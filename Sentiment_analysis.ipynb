{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabeahmed2002/Summaric/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1egNTHpuH4Rl",
        "outputId": "32535950-b2f8-40dd-9a72-1603bf90d688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t5_model_path = '/content/drive/My Drive/NLP/BART'\n",
        "roberta_model_path = '/content/drive/My Drive/NLP/Roberta'"
      ],
      "metadata": {
        "id": "HAPfA-MCLkBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# for summarization\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_path)\n",
        "\n",
        "# for sentiment analysis\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_path)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDnK87dOLozU",
        "outputId": "1f2de8d1-484d-4dec-c24b-4ab74cf55ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/My Drive/NLP/Roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=t5_model,\n",
        "    tokenizer=t5_tokenizer\n",
        ")\n",
        "\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=roberta_model,\n",
        "    tokenizer=roberta_tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iACJ82GL5-I",
        "outputId": "8cc5bbd9-3305-4ff3-fa30-7e2e4361d337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summarize_text(text):\n",
        "    # Split long texts into chunks for better summarization\n",
        "    if len(text.split()) > 100:\n",
        "        print(\"Input text is long; splitting into smaller chunks.\")\n",
        "        text_chunks = [text[i:i + 500] for i in range(0, len(text), 500)]\n",
        "        summaries = [\n",
        "            summarizer(chunk, max_length=100, min_length=50, do_sample=False)[0]['summary_text']\n",
        "            for chunk in text_chunks\n",
        "        ]\n",
        "        return \" \".join(summaries)\n",
        "    else:\n",
        "        # Summarize directly for shorter texts\n",
        "        summary = summarizer(text, max_length=100, min_length=50, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "\n",
        "\n",
        "# Sentiment analysis function\n",
        "def analyze_sentiment(text):\n",
        "    # Analyze sentiment\n",
        "    sentiment = sentiment_analyzer(text)\n",
        "\n",
        "    # Map model labels to human-readable labels\n",
        "    label_map = {\n",
        "        \"LABEL_0\": \"Negative\",\n",
        "        \"LABEL_1\": \"Neutral\",\n",
        "        \"LABEL_2\": \"Positive\"\n",
        "    }\n",
        "\n",
        "    model_label = sentiment[0]['label']\n",
        "    confidence = sentiment[0]['score']\n",
        "\n",
        "    human_readable_label = label_map.get(model_label, \"Unknown\")\n",
        "    return human_readable_label, confidence\n",
        "\n",
        "# Combined function for summarization and sentiment analysis\n",
        "def summarize_and_analyze(text):\n",
        "    print(\"Original Text:\\n\", text)\n",
        "\n",
        "    # Summarize the text\n",
        "    summarized_text = summarize_text(text)\n",
        "    print(\"\\nSummarized Text:\\n\", summarized_text)\n",
        "\n",
        "    # Perform sentiment analysis on the summarized text\n",
        "    sentiment_label, sentiment_score = analyze_sentiment(summarized_text)\n",
        "    print(\"\\nSentiment Analysis:\")\n",
        "    print(f\"Sentiment: {sentiment_label} (Confidence: {sentiment_score:.2f})\")\n",
        "    return summarized_text, sentiment_label, sentiment_score\n",
        "\n",
        "# Example usage\n",
        "text = \"\"\"\n",
        "The hotel stay was a mix of good and bad experiences. The rooms were spacious and clean, and the staff was polite. However, the food quality was subpar, and the pool area was overcrowded. Additionally, there was constant noise from nearby construction, which made relaxing difficult. Despite these issues, the location was convenient for accessing tourist attractions.\"\"\"\n",
        "summarized_text, sentiment_label, sentiment_score = summarize_and_analyze(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ugwnhDL-w7",
        "outputId": "f87621bb-9ae4-4018-8fa9-512fbf8a04c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "The hotel stay was a mix of good and bad experiences. The rooms were spacious and clean, and the staff was polite. However, the food quality was subpar, and the pool area was overcrowded. Additionally, there was constant noise from nearby construction, which made relaxing difficult. Despite these issues, the location was convenient for accessing tourist attractions.\n",
            "\n",
            "Summarized Text:\n",
            " The hotel stay was a mix of good and bad experiences. The rooms were spacious and clean, and the staff was polite. However, the food quality was subpar and the pool area was overcrowded. Despite these issues, the location was convenient for accessing tourist attractions.\n",
            "\n",
            "Sentiment Analysis:\n",
            "Sentiment: Negative (Confidence: 0.53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch sklearn\n",
        "\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Initialize Sentiment Analysis Pipeline\n",
        "def initialize_sentiment_model(roberta_model_path):\n",
        "    return pipeline(\"sentiment-analysis\", model=roberta_model_path)\n",
        "\n",
        "# Initialize Summarization Model\n",
        "def initialize_summarization_model(t5_model_path):\n",
        "    return pipeline(\"summarization\", model=t5_model_path)\n",
        "\n",
        "# Perform Sentiment Analysis\n",
        "def analyze_sentiment(sentiment_model, text):\n",
        "    sentiment = sentiment_model(text)\n",
        "    label = sentiment[0]['label']\n",
        "    score = sentiment[0]['score']\n",
        "\n",
        "    # Convert labels to Positive, Negative, Neutral for consistency\n",
        "    if label.lower() == \"positive\":\n",
        "        label = \"Positive\"\n",
        "    elif label.lower() == \"negative\":\n",
        "        label = \"Negative\"\n",
        "    else:\n",
        "        label = \"Neutral\"\n",
        "\n",
        "    return label, score\n",
        "\n",
        "# Perform Text Summarization\n",
        "def summarize_text(summarization_model, text):\n",
        "    summarized = summarization_model(text, max_length=100, min_length=50, do_sample=False)\n",
        "    return summarized[0]['summary_text']\n",
        "\n",
        "# Evaluate Model Metrics\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Define model paths\n",
        "    t5_model_path = '/content/drive/My Drive/NLP/BART'\n",
        "    roberta_model_path = '/content/drive/My Drive/NLP/Roberta'\n",
        "\n",
        "    # Initialize models\n",
        "    sentiment_model = initialize_sentiment_model(roberta_model_path)\n",
        "    summarization_model = initialize_summarization_model(t5_model_path)\n",
        "\n",
        "    # Input text for testing\n",
        "    text = \"\"\"The customer service at the restaurant was absolutely terrible. The staff were rude and inattentive, and it took ages for our order to arrive. When the food finally came, it was cold and poorly prepared, completely ruining the experience. The ambiance was also disappointing, with loud noises and a chaotic environment. Overall, it felt like a waste of money and time, and I would not recommend this place to anyone.\"\"\"\n",
        "\n",
        "    # Perform summarization\n",
        "    summarized_text = summarize_text(summarization_model, text)\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    sentiment_label, sentiment_score = analyze_sentiment(sentiment_model, text)\n",
        "\n",
        "    # Simulated ground truth for evaluation (for demonstration purposes)\n",
        "    ground_truth_sentiment = [\"Negative\"]  # Replace with actual labels if available\n",
        "    predicted_sentiment = [sentiment_label]\n",
        "\n",
        "    # Evaluate model metrics\n",
        "    accuracy, precision, recall = evaluate_model(ground_truth_sentiment, predicted_sentiment)\n",
        "\n",
        "    # Output results\n",
        "    print(\"Original Text:\\n\", text)\n",
        "    print(\"\\nSummarized Text:\\n\", summarized_text)\n",
        "    print(\"\\nSentiment Analysis:\\n\", f\"Sentiment: {sentiment_label} (Confidence: {sentiment_score:.2f})\")\n",
        "    print(\"\\nModel Evaluation Metrics:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isWmnJO_zX_8",
        "outputId": "2e92894e-9e73-40f6-9f32-dcd2e3d2d0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/My Drive/NLP/Roberta and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " The customer service at the restaurant was absolutely terrible. The staff were rude and inattentive, and it took ages for our order to arrive. When the food finally came, it was cold and poorly prepared, completely ruining the experience. The ambiance was also disappointing, with loud noises and a chaotic environment. Overall, it felt like a waste of money and time, and I would not recommend this place to anyone.\n",
            "\n",
            "Summarized Text:\n",
            " The customer service at the restaurant was absolutely terrible. The staff were rude and inattentive, and it took ages for our order to arrive. The ambiance was also disappointing, with loud noises and a chaotic environment. Overall, it felt like a waste of money and time. I would not recommend this place to anyone.\n",
            "\n",
            "Sentiment Analysis:\n",
            " Sentiment: Neutral (Confidence: 0.51)\n",
            "\n",
            "Model Evaluation Metrics:\n",
            "Accuracy: 0.00\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n"
          ]
        }
      ]
    }
  ]
}